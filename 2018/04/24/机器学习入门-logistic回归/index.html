<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>机器学习入门-线性模型（一） | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本次学习的内容是《机器学习》一书中的线性模型这一章节另外这一章节理论性的东西更多，所以还学习了《Machine Learning in Action》中的Logistic回归，实战了一些简单的例子作为互补参考。 代码以及测试用例：https://github.com/CallMeSp/MachineLearning.git 正文先说一个数学符号：arg min f(x) 是指使得函数 f(x) 取">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习入门-线性模型（一）">
<meta property="og:url" content="http://yoursite.com/2018/04/24/机器学习入门-logistic回归/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="本次学习的内容是《机器学习》一书中的线性模型这一章节另外这一章节理论性的东西更多，所以还学习了《Machine Learning in Action》中的Logistic回归，实战了一些简单的例子作为互补参考。 代码以及测试用例：https://github.com/CallMeSp/MachineLearning.git 正文先说一个数学符号：arg min f(x) 是指使得函数 f(x) 取">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?f(x)=w_1x_1+w_2x_2+...+w_dx_d+b">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?f(x)=w^Tx+b">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?w=(w_1;w_2;...;w_d)">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?D={(X_1,y_1),(X_2,y_2),...,(X_m,y_m)}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?X_i=(x_{i1};x_{i2};...;x_{id})">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?f(x_i)=w*x_i+b">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?(w,b)=arg&space;min&space;\sum(f(x_i)-y_i)^2&space;=arg&space;min&space;\sum&space;(y_i-wx_i-b)^2">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?E_w'&space;=&space;2(w*\sum(x_i^2)&space;-&space;\sum((y_i-b)x_i))">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?E_b'=&space;2(mb-\sum_{i=1}^{m}(y_i-wx_i)">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?w=\frac{\sum_{i=1}^{m}y_i(x_i-\overline{x})}{\sum_{i=1}^{m}x_i^2-\frac{1}{m}(\sum_{i=1}^{m}x_i)^2};&space;b=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?f(x_i)=w^Tx_i+b">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?X=\left[&space;\begin{matrix}&space;x_{11}&space;&&space;x_{12}&space;&&space;...&space;&x_{1d}&1\\&space;x_{21}&space;&&space;x_{12}&space;&&space;...&space;&x_{1d}&1\\&space;...&space;&&space;...&space;&&space;...&...&...\\&space;x_{m1}&x_{m2}&...&x_{md}&1&space;\end{matrix}&space;\right]&space;=\left[\begin{matrix}&space;x_1^T&1\\&space;x_2^T&1\\&space;...&...\\&space;x_m^T&1\end{matrix}&space;\right&space;]">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?y=(y_1;y_2,...;y_m)">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?w^*=arg&space;min&space;(y-Xw)^T(y-Xw)">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?E=&space;(y-Xw)^T(y-Xw)">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?E_w'=&space;2X^T(Xw-y)">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?y=\left\{\begin{matrix}&space;0,&z<0&space;\\&space;0.5,&z=0&space;\\&space;1,&z>0&space;\end{matrix}\right.">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?y=\frac{1}{1+e^{-Z}}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?y=\frac{1}{1+e^{-(w^Tx+b))}}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?ln\frac{y}{1-y}=w^Tx-b">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\frac{y}{1-y}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx+b">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}};p(y=0|x)=\frac{1}{1+e^{w^Tx+b}}">
<meta property="og:image" content="https://latex.codecogs.com/gif.latex?\imath&space;(w,b)=\sum_{i=1}^{m}lnp(y_i|x_i;w,b)">
<meta property="og:updated_time" content="2018-05-05T11:49:16.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习入门-线性模型（一）">
<meta name="twitter:description" content="本次学习的内容是《机器学习》一书中的线性模型这一章节另外这一章节理论性的东西更多，所以还学习了《Machine Learning in Action》中的Logistic回归，实战了一些简单的例子作为互补参考。 代码以及测试用例：https://github.com/CallMeSp/MachineLearning.git 正文先说一个数学符号：arg min f(x) 是指使得函数 f(x) 取">
<meta name="twitter:image" content="https://latex.codecogs.com/gif.latex?f(x)=w_1x_1+w_2x_2+...+w_dx_d+b">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-机器学习入门-logistic回归" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/04/24/机器学习入门-logistic回归/" class="article-date">
  <time datetime="2018-04-24T03:50:41.000Z" itemprop="datePublished">2018-04-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习入门-线性模型（一）
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本次学习的内容是《机器学习》一书中的线性模型这一章节另外这一章节理论性的东西更多，所以还学习了《Machine Learning in Action》中的Logistic回归，实战了一些简单的例子作为互补参考。</p>
<p>代码以及测试用例：<br><a href="https://github.com/CallMeSp/MachineLearning.git" target="_blank" rel="noopener">https://github.com/CallMeSp/MachineLearning.git</a></p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a><strong>正文</strong></h2><p>先说一个数学符号：<br>arg min f(x) 是指使得函数 f(x) 取得其最小值的所有自变量 x 的集合。比如，函数 cos(x) 在 ±π、±3π、±5π、……处取得最小值（-1），则 argmin cos(x) = {±π, ±3π, ±5π, …}。<br>如果函数 f(x) 只在一处取得其最小值，则 argmin f(x) 为单点集.</p>
<h3 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a><strong>基本形式</strong></h3><p>给定d个属性描述的示例$x=（x_1;x_2;…;x_d)$,其中xi是x在第i个属性的取值。线性模型试图学得一个通过属性的线性组合来进行预测的函数，即<br><a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=w_1x_1&plus;w_2x_2&plus;...&plus;w_dx_d&plus;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=w_1x_1&plus;w_2x_2&plus;...&plus;w_dx_d&plus;b" title="f(x)=w_1x_1+w_2x_2+...+w_dx_d+b"></a><br>用向量形式写成：<br><a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=w^Tx&plus;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)=w^Tx&plus;b" title="f(x)=w^Tx+b"></a>，其中<a href="https://www.codecogs.com/eqnedit.php?latex=w=(w_1;w_2;...;w_d)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w=(w_1;w_2;...;w_d)" title="w=(w_1;w_2;...;w_d)"></a></p>
<p>w和b学得之后，模型就得以确定。</p>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a><strong>线性回归</strong></h3><p>给定数据集<a href="https://www.codecogs.com/eqnedit.php?latex=D={(X_1,y_1),(X_2,y_2),...,(X_m,y_m)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D={(X_1,y_1),(X_2,y_2),...,(X_m,y_m)}" title="D={(X_1,y_1),(X_2,y_2),...,(X_m,y_m)}"></a>，其中<a href="https://www.codecogs.com/eqnedit.php?latex=X_i=(x_{i1};x_{i2};...;x_{id})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X_i=(x_{i1};x_{i2};...;x_{id})" title="X_i=(x_{i1};x_{i2};...;x_{id})"></a>,yi∈R 。“线性回归”试图学得一个线性模型以尽可能准确的预测实值输出标记。<br>先考虑一种最简单的情形：输入属性的数目只有一个，线性回归试图学得<br><a href="https://www.codecogs.com/eqnedit.php?latex=f(x_i)=w*x_i&plus;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x_i)=w*x_i&plus;b" title="f(x_i)=w*x_i+b"></a><br>使得f(xi)≈yi。<br>如何确定w和b呢？显然关键是如何衡量f(x)和y之间的差别，而<strong>均方误差</strong>是回归任务中最常用的性能度量，因此我们可试图让均方误差最小化，即<br><a href="https://www.codecogs.com/eqnedit.php?latex=(w,b)=arg&space;min&space;\sum(f(x_i)-y_i)^2&space;=arg&space;min&space;\sum&space;(y_i-wx_i-b)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(w,b)=arg&space;min&space;\sum(f(x_i)-y_i)^2&space;=arg&space;min&space;\sum&space;(y_i-wx_i-b)^2" title="(w,b)=arg min \sum(f(x_i)-y_i)^2 =arg min \sum (y_i-wx_i-b)^2"></a><br>均方误差有非常好的几何意义，对应了常用的欧几里得距离或简称“欧式距离”。基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。<br>为求解w和b使E=∑ (yi-wxi-b)^2最小化的过程，称为线性回归模型的最小二乘“参数估计”，我们可以将E分别对w和b求偏导。<br><a href="https://www.codecogs.com/eqnedit.php?latex=E_w'&space;=&space;2(w*\sum(x_i^2)&space;-&space;\sum((y_i-b)x_i))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E_w'&space;=&space;2(w*\sum(x_i^2)&space;-&space;\sum((y_i-b)x_i))" title="E_w' = 2(w*\sum(x_i^2) - \sum((y_i-b)x_i))"></a><br><a href="https://www.codecogs.com/eqnedit.php?latex=E_b'=&space;2(mb-\sum_{i=1}^{m}(y_i-wx_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E_b'=&space;2(mb-\sum_{i=1}^{m}(y_i-wx_i)" title="E_b'= 2(mb-\sum_{i=1}^{m}(y_i-wx_i)"></a><br>分别令其为0则可得到w和b的最优解的闭式解<br><a href="https://www.codecogs.com/eqnedit.php?latex=w=\frac{\sum_{i=1}^{m}y_i(x_i-\overline{x})}{\sum_{i=1}^{m}x_i^2*\frac{1}{m}(\sum_{i=1}^{m}x_i)^2};&space;b=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w=\frac{\sum_{i=1}^{m}y_i(x_i-\overline{x})}{\sum_{i=1}^{m}x_i^2-\frac{1}{m}(\sum_{i=1}^{m}x_i)^2};&space;b=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)" title="w=\frac{\sum_{i=1}^{m}y_i(x_i-\overline{x})}{\sum_{i=1}^{m}x_i^2*\frac{1}{m}(\sum_{i=1}^{m}x_i)^2}; b=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)"></a></p>
<p>更一般的情形是如本节开头的数据集D，样本由d个属性描述，此时试图学得<br><a href="https://www.codecogs.com/eqnedit.php?latex=f(x)=w^Tx&plus;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x_i)=w^Tx_i&plus;b" title="f(x)=w^Tx+b"></a>这称为多元线性回归。</p>
<p>类似的，可利用最小二乘法来对w和b进行估计，为便于讨论，我们把w和b吸入向量形式w=（w；b），相应的把数据集D表示为一个m<em>（d+1）大小的矩阵X，其中每一行对应于一个实例，该行前d个元素对应于示例的d个属性值，最后一个元素恒置为1即<a href="https://www.codecogs.com/eqnedit.php?latex=X=\left[&space;\begin{matrix}&space;x_{11}&space;&&space;x_{12}&space;&&space;...&space;&x_{1d}&1\\&space;x_{21}&space;&&space;x_{12}&space;&&space;...&space;&x_{1d}&1\\&space;...&space;&&space;...&space;&&space;...&...&...\\&space;x_{m1}&x_{m2}&...&x_{md}&1&space;\end{matrix}&space;\right]&space;=\left[\begin{matrix}&space;x_1^T&1\\&space;x_2^T&1\\&space;...&...\\&space;x_m^T&1\end{matrix}&space;\right&space;]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X=\left[&space;\begin{matrix}&space;x_{11}&space;&&space;x_{12}&space;&&space;...&space;&x_{1d}&1\\&space;x_{21}&space;&&space;x_{12}&space;&&space;...&space;&x_{1d}&1\\&space;...&space;&&space;...&space;&&space;...&...&...\\&space;x_{m1}&x_{m2}&...&x_{md}&1&space;\end{matrix}&space;\right]&space;=\left[\begin{matrix}&space;x_1^T&1\\&space;x_2^T&1\\&space;...&...\\&space;x_m^T&1\end{matrix}&space;\right&space;]" title="X=\left[ \begin{matrix} x_{11} & x_{12} & ... &x_{1d}&1\\ x_{21} & x_{12} & ... &x_{1d}&1\\ ... & ... & ...&...&...\\ x_{m1}&x_{m2}&...&x_{md}&1 \end{matrix} \right] =\left[\begin{matrix} x_1^T&1\\ x_2^T&1\\ ...&...\\ x_m^T&1\end{matrix} \right ]"></a><br>再把标记也写成向量形式<a href="https://www.codecogs.com/eqnedit.php?latex=y=(y_1;y_2,...;y_m)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=(y_1;y_2,...;y_m)" title="y=(y_1;y_2,...;y_m)"></a>于是类似的有<br>&lt;a href=”<a href="https://www.codecogs.com/eqnedit.php?latex=w^" target="_blank" rel="noopener">https://www.codecogs.com/eqnedit.php?latex=w^</a></em>=argmin(y-Xw)^T(y-Xw)” target=”_blank”&gt;<img src="https://latex.codecogs.com/gif.latex?w^*=arg&space;min&space;(y-Xw)^T(y-Xw)" title="w^*=arg min (y-Xw)^T(y-Xw)">,令<a href="https://www.codecogs.com/eqnedit.php?latex=E=&space;(y-Xw)^T(y-Xw)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E=&space;(y-Xw)^T(y-Xw)" title="E= (y-Xw)^T(y-Xw)"></a>,对w求导得<a href="https://www.codecogs.com/eqnedit.php?latex=E_w'=&space;2X^T(Xw-y)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E_w'=&space;2X^T(Xw-y)" title="E_w'= 2X^T(Xw-y)"></a><br>令上式为0可得w最优解的闭式解。</p>
<h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a><strong>对数几率回归</strong></h3><p>考虑二分任务，其输出标记y∈{0,1}，而线性回归模型产生的预测值是实值，所以需要将实值z转换为0/1值。最理想的是单位阶跃函数<br><a href="https://www.codecogs.com/eqnedit.php?latex=y=\left\{\begin{matrix}&space;0,&z<0&space;\\&space;0.5,&z=0&space;\\&space;1,&z>0&space;\end{matrix}\right." target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=\left\{\begin{matrix}&space;0,&z<0&space;\\&space;0.5,&z=0&space;\\&space;1,&z>0&space;\end{matrix}\right." title="y=\left\{\begin{matrix} 0,&z<0 \\ 0.5,&z=0 \\ 1,&z>0 \end{matrix}\right."></a><br>但是该函数不连续，所以需要找到一个函数一定程度上近似单位阶跃函数，并且单点课为，对数几率函数（logistic function）正是这样一个常用替代函数：<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{1}{1&plus;e^{-z}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=\frac{1}{1&plus;e^{-Z}}" title="\frac{1}{1+e^{-z}}"></a><br>从而得到:<a href="https://www.codecogs.com/eqnedit.php?latex=y=\frac{1}{1&plus;e^{-(w^Tx&plus;b))}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y=\frac{1}{1&plus;e^{-(w^Tx&plus;b))}}" title="y=\frac{1}{1+e^{-(w^Tx+b))}}"></a>（其对应模型称为对数几率回归，实际是一种分类学习方法）<br>于是可进一步变化为：<a href="https://www.codecogs.com/eqnedit.php?latex=ln\frac{y}{1-y}=w^Tx-b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?ln\frac{y}{1-y}=w^Tx-b" title="ln\frac{y}{1-y}=w^Tx-b"></a><br>若将y视为样本x作为正例的可能性，则1-y是其反例的可能性，两者的比值<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{y}{1-y}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{y}{1-y}" title="\frac{y}{1-y}"></a>称为“几率”，取对数则得到对数几率。</p>
<h5 id="确定w和b"><a href="#确定w和b" class="headerlink" title="确定w和b"></a><strong>确定w和b</strong></h5><p>将y视为后验概率p（y=1|x）则可将上式重写为<a href="https://www.codecogs.com/eqnedit.php?latex=ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx&plus;b" target="_blank"><img src="https://latex.codecogs.com/gif.latex?ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx&plus;b" title="ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx+b"></a><br>显然有<a href="https://www.codecogs.com/eqnedit.php?latex=p(y=1|x)=\frac{e^{w^Tx&plus;b}}{1&plus;e^{w^Tx&plus;b}};p(y=0|x)=\frac{1}{1&plus;e^{w^Tx&plus;b}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p(y=1|x)=\frac{e^{w^Tx&plus;b}}{1&plus;e^{w^Tx&plus;b}};p(y=0|x)=\frac{1}{1&plus;e^{w^Tx&plus;b}}" title="p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}};p(y=0|x)=\frac{1}{1+e^{w^Tx+b}}"></a>于是可以通过极大似然法来估计w和b:      <a href="https://www.codecogs.com/eqnedit.php?latex=\imath&space;(w,b)=\sum_{i=1}^{m}lnp(y_i|x_i;w,b)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\imath&space;(w,b)=\sum_{i=1}^{m}lnp(y_i|x_i;w,b)" title="\imath (w,b)=\sum_{i=1}^{m}lnp(y_i|x_i;w,b)"></a></p>
<h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a><strong>实战</strong></h3><p>logistic回归实战。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataMat=[]</span><br><span class="line">    labelMat=[]</span><br><span class="line">    fr=open(<span class="string">'testSet.txt'</span>,<span class="string">'r'</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr=line.strip().split()</span><br><span class="line">        <span class="comment"># 每行前两个值作为x1，x2，另外加一个x0，设为1.0</span></span><br><span class="line">        dataMat.append([<span class="number">1.0</span>,float(lineArr[<span class="number">0</span>]),float(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(int(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(inX)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+exp(-inX))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMatIn,classLabels)</span>:</span></span><br><span class="line">    <span class="comment"># 将普通矩阵转化为numpy矩阵数据类型</span></span><br><span class="line">    dataMatrix=mat(dataMatIn)</span><br><span class="line">    <span class="comment"># 矩阵的转置，转化为列矩阵</span></span><br><span class="line">    labelMat=mat(classLabels).transpose()</span><br><span class="line">    m,n=shape(dataMatrix)</span><br><span class="line">    alepha=<span class="number">0.001</span></span><br><span class="line">    maxCycles=<span class="number">500</span></span><br><span class="line">    weights=ones((n,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(maxCycles):</span><br><span class="line">        h=sigmoid(dataMatrix*weights)</span><br><span class="line">        error=(labelMat-h)</span><br><span class="line">        weights=weights+alepha*dataMatrix.transpose()*error</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span><span class="params">(weights)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    dataMat,labelMat=loadDataSet()</span><br><span class="line">    dataArr=array(dataMat)</span><br><span class="line">    n = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    xcord1=[];ycord1=[]</span><br><span class="line">    xcord2=[];ycord2=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> int(labelMat[i])==<span class="number">1</span>:</span><br><span class="line">            xcord1.append(dataArr[i,<span class="number">1</span>]);ycord1.append(dataArr[i,<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xcord2.append(dataArr[i,<span class="number">1</span>]);ycord2.append(dataArr[i,<span class="number">2</span>])</span><br><span class="line">    fig=plt.figure()</span><br><span class="line">    ax=fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.scatter(xcord1,ycord1,s=<span class="number">30</span>,c=<span class="string">'red'</span>,marker=<span class="string">'s'</span>)</span><br><span class="line">    ax.scatter(xcord2,ycord2,s=<span class="number">30</span>,c=<span class="string">'blue'</span>)</span><br><span class="line">    x=arange(<span class="number">-3.0</span>,<span class="number">3.0</span>,<span class="number">0.1</span>)</span><br><span class="line">    y=(-weights[<span class="number">0</span>]-weights[<span class="number">1</span>]*x)/weights[<span class="number">2</span>]</span><br><span class="line">    <span class="comment"># to make x and y's shape be same ,transpost y</span></span><br><span class="line">    ax.plot(x,y.transpose())</span><br><span class="line">    plt.xlabel(<span class="string">'X1'</span>);plt.ylabel(<span class="string">'X2'</span>);</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataArr,classLabels)</span>:</span></span><br><span class="line">    m,n=shape(dataArr)</span><br><span class="line">    alpha=<span class="number">0.01</span></span><br><span class="line">    weights=ones(n)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        h=sigmoid(sum(dataArr[i]*weights))</span><br><span class="line">        error=classLabels[i]-h</span><br><span class="line">        weights=weights*alpha*error*dataArr[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataArr, classLabels, numIter=<span class="number">150</span>)</span>:</span></span><br><span class="line">    m,n = shape(dataArr)                                                <span class="comment">#返回dataMatrix的大小。m为行数,n为列数。</span></span><br><span class="line">    weights = ones(n)                                                       <span class="comment">#参数初始化</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">        dataIndex = list(range(m))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            alpha = <span class="number">4</span>/(<span class="number">1.0</span>+j+i)+<span class="number">0.01</span>                                            <span class="comment">#降低alpha的大小，每次减小1/(j+i)。</span></span><br><span class="line">            randIndex = int(random.uniform(<span class="number">0</span>,len(dataIndex)))                <span class="comment">#随机选取样本</span></span><br><span class="line">            h = sigmoid(sum(dataArr[randIndex]*weights))                    <span class="comment">#选择随机选取的一个样本，计算h</span></span><br><span class="line">            error = classLabels[randIndex] - h                                 <span class="comment">#计算误差</span></span><br><span class="line">            weights = weights + alpha * error * dataArr[randIndex]       <span class="comment">#更新回归系数</span></span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])                                         <span class="comment">#删除已经使用的样本</span></span><br><span class="line">    <span class="keyword">return</span> weights </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyVector</span><span class="params">(inX,weights)</span>:</span></span><br><span class="line">    prob=sigmoid(sum(inX*weights))</span><br><span class="line">    <span class="keyword">if</span> prob&gt;<span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colicTest</span><span class="params">()</span>:</span></span><br><span class="line">    frTrain=open(<span class="string">'horseColicTraining.txt'</span>)</span><br><span class="line">    frTest=open(<span class="string">'horseColicTest.txt'</span>)</span><br><span class="line">    trainingSet=[]</span><br><span class="line">    trainingLabels=[]</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTrain.readlines():</span><br><span class="line">        currLine=line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        trainingSet.append(lineArr)</span><br><span class="line">        trainingLabels.append(float(currLine[<span class="number">21</span>]))</span><br><span class="line">    trainWeights=stocGradAscent1(array(trainingSet),trainingLabels,<span class="number">500</span>)</span><br><span class="line">    errorCount=<span class="number">0</span></span><br><span class="line">    numTestVec=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTest.readlines():</span><br><span class="line">        numTestVec+=<span class="number">1.0</span></span><br><span class="line">        currLine=line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr=[]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        <span class="keyword">if</span> int(classifyVector(array(lineArr),trainWeights))!=int(currLine[<span class="number">21</span>]):</span><br><span class="line">            errorCount+=<span class="number">1</span></span><br><span class="line">    errorRate=float(errorCount)/numTestVec</span><br><span class="line">    print(<span class="string">'the error count is : %f'</span>%errorRate)</span><br><span class="line">    <span class="keyword">return</span> errorRate</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiTest</span><span class="params">()</span>:</span></span><br><span class="line">    numTest=<span class="number">10</span></span><br><span class="line">    errorSum=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTest):</span><br><span class="line">        errorSum+=colicTest()</span><br><span class="line">    print(<span class="string">'after %d iterations the average error rate is %f'</span>%(numTest,errorSum/float(numTest)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(__name__==<span class="string">'__main__'</span>):</span><br><span class="line">    <span class="comment"># dataArr,labelMat=loadDataSet()</span></span><br><span class="line">    <span class="comment"># weights=stocGradAscent1(array(dataArr),labelMat)</span></span><br><span class="line">    <span class="comment"># print(weights)</span></span><br><span class="line">    <span class="comment"># plotBestFit(weights)</span></span><br><span class="line">    multiTest(）</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/04/24/机器学习入门-logistic回归/" data-id="cjghww1rt00001ru8qqkryips" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2018/04/23/机器学习入门-朴素贝叶斯/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">机器学习入门-贝叶斯分类器（一）</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/04/24/机器学习入门-logistic回归/">机器学习入门-线性模型（一）</a>
          </li>
        
          <li>
            <a href="/2018/04/23/机器学习入门-朴素贝叶斯/">机器学习入门-贝叶斯分类器（一）</a>
          </li>
        
          <li>
            <a href="/2018/04/19/机器学习入门-决策树/">机器学习入门-决策树（一）</a>
          </li>
        
          <li>
            <a href="/2018/04/19/机器学习入门-kNN算法/">机器学习入门-kNN算法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>